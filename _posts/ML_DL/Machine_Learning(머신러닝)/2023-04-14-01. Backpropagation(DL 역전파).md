---
title: "01. Backpropagation(DL 역전파)"
categories: Machine_Learning
tag: [python, machine_learning, concept]
toc: true
toc_sticky: true
author_profile: false
sidebar:
  nav: "docs"
typora-root-url: ../../..
---





# Backpropagation

**MLP 포스팅 게시글과 유사한 내용이긴 하다.**

<br><br>

## 1. Perceptron + 뉴런(노드) Process 

**뇌세포 == 뉴런(노드)** 

**wx+b => Perceptron**

![image-20230419215708787](/images/2023-04-14-01. Backpropagation(DL 역전파)/image-20230419215708787.png) 

* SLP : 히든레이어 X

<br>

### Perceptron 의 행렬곱

 ![1](/images/2023-04-14-01. Backpropagation(DL 역전파)/1.png) 

<br><br>

## 2. Layer(레이어)

**딥러닝(DL=DNN)이란 히든 레이어를 많이(깊게) 쌓은 MLP, CNN 등등 이런 구조를 칭한다.**

![image-20230419220004853](/images/2023-04-14-01. Backpropagation(DL 역전파)/image-20230419220004853.png) 

<br><br>

## 3. Gradient Decent Method(GD method)

**아주 유명한 "경사하강법" 이며, 특정값을 찾아가는 방법이다.**

**초기값 W를 학습시켜서  "손실 함수"가 최소값(특정값)이 되는 W파라미터를 구해준다.**

![image-20230419220748737](/images/2023-04-14-01. Backpropagation(DL 역전파)/image-20230419220748737.png) 

<br><br>

## 4. Loss(Cost) Function

**손실(비용,오차) 함수이므로 당연히 "최소화"가 목적이다.**

**"최소화" 를 위해서 경사하강법을 적용할 수 있다.**

![image-20230419220916875](/images/2023-04-14-01. Backpropagation(DL 역전파)/image-20230419220916875.png) 

<br><br>

## 5. Chain Rule과 Activation Function

**여기서 Activation Function는 `-4x` 로 볼 수 있다.**

![image-20230419221445676](/images/2023-04-14-01. Backpropagation(DL 역전파)/image-20230419221445676.png) 

<br>

### 예제 1

![image-20230419221913700](/images/2023-04-14-01. Backpropagation(DL 역전파)/image-20230419221913700.png) 

<br>

### 예제 2

![image-20230419221933802](/images/2023-04-14-01. Backpropagation(DL 역전파)/image-20230419221933802.png) 

<br>

### 예제 3

![1](/images/2023-04-14-01. Backpropagation(DL 역전파)/1-16819105797211.png) 
